{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58768f7d-1138-477e-92c1-9c692954cad0",
   "metadata": {},
   "source": [
    "# Suivis Projet Info : Stellar Analytics\n",
    "#### Axelle Refeyton . Jean Roelens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2086a81-d853-428c-b0a8-26839441c19c",
   "metadata": {},
   "source": [
    "Le but de se projet est initialement de permettre à un utilisateur de prendre une photo d'une étoile et de retrouver des informations dessus, notamment sont nom, sa position ainsi que sa composition chimique qui peux être récupérer en fonction de son spectre lumineux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af42a8-2fb6-4ef2-a41b-98cb2ecd6808",
   "metadata": {},
   "source": [
    "Ressources utilisées : \n",
    "- [NIST Spectra Database](https://physics.nist.gov/PhysRefData/ASD/lines_form.html)\n",
    "- [Documentation sur les spectres lumineux](https://atomic-spectra.net/)\n",
    "- [Simbad, DB d'étoiles communautaire open source](https://simbad.u-strasbg.fr/simbad/)\n",
    "- [MAST portal](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html)\n",
    "- [Raies de Fraunhofer](https://fr.wikipedia.org/wiki/Raies_de_Fraunhofer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6325db-18bc-4b28-b676-bc19ea9c5709",
   "metadata": {},
   "source": [
    "En début de projet beaucoup de temps fut consacré à de la recherche de documentation ainsi que de jeux de données. Egalement nous avons eu l'occasion de nous entretenir lors d'une conférence Zoom avec deux membres du CNES (Orphée Faucoz et Denis Standarovski) travaillant depuis plusieurs années sur un sujet similaire : L'identification d'exoplanétes en utilisant les spectres d'absorption.\n",
    "\n",
    "Au cours de l'entretien ils nous ont clairement expliqué que ce que l'on souhaiter faire êtait une tâche difficile et que cela nécessité du temps ne serait-ce que pour la compréhension de l'environnement dans lequel nous naviguons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fc5e2-2628-441a-87cd-2f9d22d8b6e8",
   "metadata": {},
   "source": [
    "On a parcouru beaucoup de base de données gratuite et d'autres non afin de trouver celles qui correspondrait au mieux à nos besoins. Au final on a retenue Simbad pour la récupération des spectres émis par les étoiles et NIST pour récupérer le spectre d'émissions des élèments que peuvent composer une étoile, càd : Oxygéne, Hydrogénes, Sodium, Hélium, Mercure, Fer, Magnésium, Calcium, Titane et finalement Nickel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a24c9c-0c4e-44ef-b8a0-8c14af6a3864",
   "metadata": {},
   "source": [
    "à partir de ce moment on a divisé la structure du projet en deux parties, une partie objet céleste (gérer par Axelle Refeyton) et une partie spectre chimique (gérer par Jean Roelens). Ensuite chaque partie est divisés en étapes afin de faciliter le procéder de réalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac218d2a-038a-451d-8f66-eecd2f59a138",
   "metadata": {},
   "source": [
    "## Spectre Chimique (J.Roelens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a406d77-7e3a-41ee-b726-c2fa84c8570c",
   "metadata": {},
   "source": [
    "L'objectif de cette partie est :\n",
    "1. **Pour chaque élèments récupérer les différentes ionization possible.**\n",
    "2. **Extraire le tableau de spectre depuis NIST (en utilisant un scrapper)**\n",
    "3. **Le formatter pour récupérer que les lignes fortes**\n",
    "4. **Créer un dataset par élément et ionization avec les lignes fortes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966b437-d2a5-4089-a17e-b215984f6c32",
   "metadata": {},
   "source": [
    "Pour commencer il faut que je récupére les ionizations de chaque élèments, étant une étape pas forcément très complexe j'ai préféré le faire à la main en m'aidant de [atomic-spectra](https://atomic-spectra.net/spectrum.php) et des [Raies de Fraunhofer](https://fr.wikipedia.org/wiki/Raies_de_Fraunhofer) On en sort cette liste : \n",
    "- Oxygéne [I, II, III, IV, V, VI, VII, VIII]\n",
    "- Hydrogénes [I]\n",
    "- Sodium [I, II, III, IV, V, VI, VII, VIII, IX, X, XI]\n",
    "- Hélium [I, II]\n",
    "- Mercure [I, II, III]\n",
    "- Fer [I, II ,III , IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI, XVII, XVIII, XIX, XX, XXI, XXII, XXIII, XXIV, XXV, XVI]\n",
    "- Magnésium [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII]\n",
    "- Calcium [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI]\n",
    "- Titane [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI, XVII, XVIII, XIX, XX, XXI, XXII]\n",
    "- Nickel [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI, XVII, XVIII, XIX, XX, XXI, XXII, XXIII, XXIV, XXV, XXVI, XXVII, XXVIII]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3d45b-1c60-44aa-8a72-110b98ef1fe0",
   "metadata": {},
   "source": [
    "Maintenant que l'étape facile est faites je vais pouvoir m'attaquer à l'étape plus hardu d'extraire les données depuis NIST. L'idée global dérriére est de récupérer le HTML de la page NIST en donnant dans l'URL du site l'élèment ainsi que son indice de ionization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64242ce2-f0d6-48bb-a0e0-e8622f0c863b",
   "metadata": {},
   "source": [
    "En saisissant H I (pour Hydrogénes Ionization 1) on obtiens cet URL :\n",
    "\n",
    "https://physics.nist.gov/cgi-bin/ASD/lines1.pl?spectra=H+I&output_type=0&low_w=&upp_w=&unit=1&submit=Retrieve+Data&de=0&plot_out=0&I_scale_type=1&format=0&line_out=0&en_unit=0&output=0&bibrefs=1&page_size=15&show_obs_wl=1&show_calc_wl=1&unc_out=1&order_out=0&max_low_enrg=&show_av=2&max_upp_enrg=&tsb_value=0&min_str=&A_out=0&intens_out=on&max_str=&allowed_out=1&forbid_out=1&min_accur=&min_intens=&conf_out=on&term_out=on&enrg_out=on&J_out=on\n",
    "\n",
    "Plutôt long, mais ce qui est assez intéressant que beaucoup de paramètres pourront-être passé assez facilement par celles-ci comparons avec l'URL de O I pour voir ou nous devrons saisir l'élèment et l'ionization \n",
    "\n",
    "https://physics.nist.gov/cgi-bin/ASD/lines1.pl?spectra=O+I&output_type=0&low_w=&upp_w=&unit=1&submit=Retrieve+Data&de=0&plot_out=0&I_scale_type=1&format=0&line_out=0&en_unit=0&output=0&bibrefs=1&page_size=15&show_obs_wl=1&show_calc_wl=1&unc_out=1&order_out=0&max_low_enrg=&show_av=2&max_upp_enrg=&tsb_value=0&min_str=&A_out=0&intens_out=on&max_str=&allowed_out=1&forbid_out=1&min_accur=&min_intens=&conf_out=on&term_out=on&enrg_out=on&J_out=on\n",
    "\n",
    "Comme on peut le voir l'élèment se trouve en début d'URL essayons cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea233f17-d499-4df2-89d1-5b2b3309b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succés de la requéte\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "START_URL = r\"https://physics.nist.gov/cgi-bin/ASD/lines1.pl?spectra=\"\n",
    "END_URL = r\"&output_type=0&low_w=&upp_w=&unit=1&submit=Retrieve+Data&de=0&plot_out=0&I_scale_type=1&format=0&line_out=0&en_unit=0&output=0&bibrefs=1&page_size=15&show_obs_wl=1&show_calc_wl=1&unc_out=1&order_out=0&max_low_enrg=&show_av=2&max_upp_enrg=&tsb_value=0&min_str=&A_out=0&intens_out=on&max_str=&allowed_out=1&forbid_out=1&min_accur=&min_intens=&conf_out=on&term_out=on&enrg_out=on&J_out=on\"\n",
    "\n",
    "# Essayons de voir si on peut obtenir un code 200, mais avant cela construisons notre URL de maniére naïve pour ce test\n",
    "\n",
    "URL = START_URL + \"Ni+X\" + END_URL\n",
    "\n",
    "resp = requests.get(URL)\n",
    "if resp.status_code == 200:\n",
    "    print(f\"Succés de la requéte\")\n",
    "else :\n",
    "    print(f\"Echec de la requéte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f9639-5a69-4c45-880c-36843c45eb3a",
   "metadata": {},
   "source": [
    "Maintenant que notre approche naïve fonctionne vérifions que l'intégralité des élèments et ionization fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac395e4a-0267-466a-87a6-86253e94f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le fait que je marque certaines \"variables\" en majuscules est simplement un tic pour m'indiquer qu'il s'agit de constante\n",
    "ELEMENT_DICT = {\n",
    "    \"O\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\"],\n",
    "    \"H\" : [\"I\"],\n",
    "    \"Na\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\"],\n",
    "    \"He\" : [\"I\",\"II\"],\n",
    "    \"Hg\" : [\"I\",\"II\",\"III\"],\n",
    "    \"Fe\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\"],\n",
    "    \"Mg\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\"],\n",
    "    \"Ca\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\"],\n",
    "    \"Ti\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\",\"XVII\",\"XVIII\",\"XIX\",\"XX\",\"XXI\",\"XXII\"],\n",
    "    \"Ni\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\",\"XVII\",\"XVIII\",\"XIX\",\"XX\",\"XXI\",\"XXII\",\"XXIII\",\"XXIV\",\"XXV\",\"XXVI\",\"XXVII\",\"XXVIII\"]\n",
    "}\n",
    "\n",
    "# Je vérifie la longueur de chacune des clès pour être sur de ne pas avoir fait d'erreur \n",
    "def element_dict_verification():\n",
    "    for key in ELEMENT_DICT.keys():\n",
    "        print(f\"Elément : {key} Len : {len(ELEMENT_DICT[key])}\")\n",
    "\n",
    "# Et ensuite on test les requêtes\n",
    "\n",
    "def get_200(r : requests.models.Response):\n",
    "    if r.status_code != 200:\n",
    "        return False\n",
    "    return True\n",
    "def multi_request_verification(d : dict):\n",
    "    for k in d.keys():\n",
    "        for v in range(len(d[k])):\n",
    "            url = START_URL + k + \"+\" + d[k][v] + END_URL\n",
    "            if not get_200(requests.get(url)):\n",
    "                print(f\"Error with el : {k} and ionization : {d[k][v]}\")\n",
    "            else : \n",
    "                print(f\"Success with el : {k} and ionization : {d[k][v]}\")\n",
    "# multi_request_verification(ELEMENT_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627647d4-82fe-42b8-8a00-f0e6ccbf6967",
   "metadata": {},
   "source": [
    "Ok on dirait bien qu'on a que des codes 200 pour l'ensemble des requêtes ! Malheuresement on se rendra compte plus tard que NIST renvoie des code 200 même en cas de mauvais URL. Le seul moyen de savoir si la requête a bien aboutit et de récupérer le tableau **\"\\<table>\"** qui va contenir les données qui nous intéresse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21043a9-2811-486b-98cd-4ba61a3f02ff",
   "metadata": {},
   "source": [
    "L'idée va être de voir si on peut récupérer la balise \"table\" contenant le styling que nous cherchons pour chaque lien avec l'aide de la librairie que nous allons le plus utiliser durant notre partie : \"BeautifulSoup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9296ef13-88de-40be-910d-ef301b2ee57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "# Vu que je vais beaucoup utiliser request autant se créer des petites fonctions QOL (Quality Of Life)\n",
    "\n",
    "def url_builder(d : dict):\n",
    "    resp = {}\n",
    "    for key in d.keys():\n",
    "        for v in d[key]:\n",
    "            resp[key+v] = START_URL + key + \"+\" + v + END_URL\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f595db-6b41-43ab-a833-49a9f70a04d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with url : NiVI\n",
      "error with url : NiVIII\n"
     ]
    }
   ],
   "source": [
    "urls = url_builder(ELEMENT_DICT)\n",
    "for url in urls:\n",
    "    req = requests.get(urls[url])\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    target_table = [table for table in tables if 'background-color:#FFFEEE;' in table.get('style', '')]\n",
    "    if len(target_table) != 1:\n",
    "        print(f'error with url : {url}')\n",
    "    else :\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71520cd3-a275-4eb9-9401-4b0308d83023",
   "metadata": {},
   "source": [
    "On voit donc désormais que deux lien sont *mort* celui pour le Nickel 6 et le Nickel 8. Hormis ça l'ensemble des autres liens marche. Pour la prochaine étape il nous faut récupérer toute les rows contenant une donnée dans la colonne *Observed Wavelength Vac (nm)* ainsi que *Rel. Int.* C'est les deux colonnes qui nous intéresse pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388a7a5-8be8-4377-b7ad-1b8203d6dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       wavelength intensities chemical\n",
      "count         174         174      174\n",
      "unique        157          48        1\n",
      "top       94.8686         120      O I\n",
      "freq            3          18      174\n"
     ]
    }
   ],
   "source": [
    "# Pour commencer passont la logique précédente en fonction \n",
    "\n",
    "def get_table(req : requests.models.Response):\n",
    "    soup = BeautifulSoup(req.text,'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    target_table = [table for table in tables if 'background-color:#FFFEEE;' in table.get('style', '')]\n",
    "    if len(target_table) > 0 :\n",
    "        return target_table[0]\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "# On va traiter seulement le premier question de rapidité depuis python 3.6 les dicts sont ordonnées du coup le premier élèment sera toujours OI\n",
    "urls = url_builder(ELEMENT_DICT)\n",
    "oxygen_one = list(urls.items())[0]\n",
    "table = get_table(requests.get(oxygen_one[1]))\n",
    "# Maintenant on va \"drill down\" la table afin d'obtenir tout les Tbody puis les Tr qui nous intéressent.\n",
    "tbodies = table.find_all('tbody')\n",
    "# Seul les tbody pair nous intéresse car ils contiennent la data en question\n",
    "evenTBodies = tbodies[1::2]\n",
    "# Ensuite pour chaque tbody on veut récupérer toute les balises tr\n",
    "# firstTBody = evenTBodies[0]\n",
    "# Holder pour les values récupéré afin d'en faire un dataframe par la suite\n",
    "All_Observed_Wavelength = []\n",
    "All_Relative_Intensities = []\n",
    "\n",
    "# for tbody in evenTBodies :\n",
    "#     for tr in tbody.find_all('tr'):\n",
    "#         all_td = tr.find_all('td')\n",
    "#         if len(all_td) >= 2:\n",
    "#             Obs_Wav = all_td[0].text.strip()\n",
    "#             Rel_Int = all_td[2].text.strip()\n",
    "#             if Obs_Wav != \"\" and Rel_Int != \"\":\n",
    "#                 All_Observed_Wavelength.append(Obs_Wav)\n",
    "#                 All_Relative_Intensities.append(Rel_Int)\n",
    "# pprint(All_Observed_Wavelength)\n",
    "\n",
    "# Tiens on remarque que la structure du HTML doit avoir un petite erreur car lors du print de All_Observed_Wavelength \n",
    "# on remarque qu'on print les bonnes colonnes avec les bonnes valeur mais, on a 1 fois le tableau 1, 2 fois le tableau 2, et finalement 3 fois le tableau\n",
    "# 3 ce qui nous indique que l'on peut parcourir une seul fois le tableau 1 et récupérer toutes les données.\n",
    "\n",
    "tbody = tbodies[1]\n",
    "for tr in tbody.find_all('tr'):\n",
    "    all_td = tr.find_all('td')\n",
    "    if len(all_td) >= 2:\n",
    "        obs_wav = all_td[0].text.strip()\n",
    "        rel_int = all_td[2].text.strip()\n",
    "        if obs_wav != \"\" and rel_int != \"\":\n",
    "            All_Observed_Wavelength.append(obs_wav)\n",
    "            All_Relative_Intensities.append(rel_int)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'wavelength' : All_Observed_Wavelength,\n",
    "    'intensities': All_Relative_Intensities,\n",
    "    'chemical' : 'O I'\n",
    "})\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f12ba1-c599-458f-9b82-a09f45378a7a",
   "metadata": {},
   "source": [
    "## Création de spectre (A.Refeyton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946efe2-b52b-45aa-86eb-697f04d9fb8d",
   "metadata": {},
   "source": [
    "<font color='#FF5733'>Attention cette partie comportera des blocs de code mais ceux-ci seront plus à titre indicatif que réellement fonctionnel étant donné le besoin des jeux de donnèes pour que ceux-ci fonctionne.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255f917-2853-4cb7-8b18-395d62f3be7d",
   "metadata": {},
   "source": [
    "Pour la création de spectre l'idée va être de re exploiter les donnèes scrapper de maniére à sélectionner parmie elles un nombre X d'éléments et de créée une sorte de copycat de spectres d'étoiles à partir de ces élèments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042be55-6de8-4d7e-acd7-f29fca354726",
   "metadata": {},
   "source": [
    "Pour commencer il nous faut récupérer la liste des éléments, ceux ci sont stocké directement dans le nom des csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "996ac0b2-5ff5-4c79-853c-fccc79aaafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_elements_list(path):\n",
    "    \"\"\"\n",
    "    path : chemin vers le dossier contenant tout les csv éléments\n",
    "    \"\"\"\n",
    "    filenames = os.listdir(path)\n",
    "    dataframes = []\n",
    "    for n in filenames:\n",
    "        data = pd.read_csv(path + \"\\\\\" + n)\n",
    "        # On nomme le dataframe en fonction de l'élément\n",
    "        data.Name = n[:-4]\n",
    "        dataframes.append(data)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f4dd8-5d98-4884-a301-bfc9c14fad7e",
   "metadata": {},
   "source": [
    "Une fois les dataframes récupérer et nommé on va pouvoir donc les exploiter et générer nos spectres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52b935e7-83e4-4484-8ada-9e76e950ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "def generate_spectrums(base_elements, n):\n",
    "    \"\"\"\n",
    "    base_elements : Ensemble des dataframes éléments\n",
    "    n : nombre de dataframe à générer\n",
    "    \"\"\"\n",
    "    max_elems = len(base_elements)\n",
    "    list_df = []\n",
    "    matrice_labels = pd.DataFrame(columns=[x.Name for x in base_elements]) \n",
    "\n",
    "    # On vérifie que plus de 2 éléments soit présent\n",
    "    if max_elems < 2:\n",
    "        raise ValueError(f'Not enough data, {max_elems} elements present')\n",
    "    for i in range(n):\n",
    "        # création du df resultat en y ajoutant un nombre aléatoire d'éléments de base compris entre 2 et 5\n",
    "        rand_elems = rand.choices(base_elements, k=rand.randrange(2, max_elems if max_elems < 5 else 5))\n",
    "        # initialisation des columns\n",
    "        matrice_labels.loc[i] = [0] * max_elems\n",
    "\n",
    "        # Changement du 0 en 1 pour les éléments présents\n",
    "        for elem in rand_elems:\n",
    "            matrice_labels.at[i, elem.Name] = 1\n",
    "            \n",
    "        df_res = pd.concat(rand_elems, ignore_index=True)\n",
    "\n",
    "        # Génération du bruit\n",
    "        n_noise = rand.randint(1500, 2000)\n",
    "        # Génération des wavelengths pour chacun des bruits\n",
    "        rand_wls = [rand.uniform(0,1100) for x in range(n_noise)]\n",
    "\n",
    "        # Ajout des noise au dataset existant\n",
    "        for n in range(0,n_noise):\n",
    "            df_res.loc[len(df_res)] = {\"wavelength\": round(rand_wls[n], 1), \"relint\": round(rand.uniform(0, 50), 1)} \n",
    "        \n",
    "        list_df.append(df_res.sort_values(\"wavelength\"))\n",
    "        \n",
    "    return list_df, matrice_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad9c21-9c29-4d7d-ba35-f756f3e29a66",
   "metadata": {},
   "source": [
    "Nous avons donc maintenant des spectres fabriqués par nos soins avec du bruit de maniére à rendre le résultat plus proche du réel.\n",
    "Il est temps de l'exploiter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bac455-bff9-4643-bb58-4da5843d6030",
   "metadata": {},
   "source": [
    "## Exploitation des spectres construis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48889965-5734-49f3-b1c3-5bfd932d229c",
   "metadata": {},
   "source": [
    "Attaquons nous à la partie qui nous intéresse, exploiter les spectres de maniére à voir si on est capable de trouver les différents éléments d'un spectre avec un modèle.\n",
    "\n",
    "Pour ça nous allons devoir utiliser un type de modèle un peu particulier, celui de multi-labelling classification. Il s'agit de modèle capable de classifier une observation selon plusieurs labels.\n",
    "\n",
    "La premiére approche va être avec un modéle unique puis nous verrons avec plusieurs modéle en même temps (pour un gain de temps).\n",
    "\n",
    "Mais avant d'exploiter les modèles il nous faut encore quelques fonctions qui vont nous servir pour tout ce qui va être reshaping des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f025940-2537-4e6d-a747-2e50469c0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import warnings # Permet d'ignorer certains warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77238f9-2433-4a79-8941-68054b131daf",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser le PCA (Principal Component Analysis) qui nous permettra d'isoler les informations de spectres qui nous intéresse, et gagner en temps de process. Il faut d'abord que notre spectre soit interprété comme un array de dimension 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459702ee-c7c6-48da-8deb-5679256464ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(list_df):\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    for df in list_df:\n",
    "        # Aplatir le df en liste de x valeurs\n",
    "        data_flat = df.to_numpy().flatten()\n",
    "\n",
    "        # ajouter des colonnes au dataframe s'il n'y en a pas assez\n",
    "        while len(data_flat) > len(result_df.columns):\n",
    "            result_df[len(result_df.columns)] = 0\n",
    "\n",
    "        # Si le df aplatis à moins de colonnes que le df de résultat on rajoute des colonnes au df aplatis et on y attribue la valeur nan (0)\n",
    "        if len(data_flat) < len(result_df.columns):\n",
    "            data_flat = np.concatenate([data_flat, [0] * (len(result_df.columns) - len(data_flat))])\n",
    "        \n",
    "        # ajout de la ligne\n",
    "        result_df.loc[len(result_df)] = data_flat\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916411c-5bef-48e5-b7c2-08d6b88aa47e",
   "metadata": {},
   "source": [
    "Maintenant nous allons pouvoir mettre en place notre PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24b1278-b5cd-4108-b4eb-938832ee330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(df, nb_components):\n",
    "\n",
    "    # Transforme le dataframe pour que les valeurs soit comprise en 0 et 1\n",
    "    data_scaled = MinMaxScaler().fit(df).transform(df)\n",
    "\n",
    "    # PCA \n",
    "    result_df = PCA(n_components=nb_components).fit_transform(data_scaled)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc78fd-b9c3-4a0c-9c3a-8e70081c3a08",
   "metadata": {},
   "source": [
    "Pas très impressionant en effet et pourtant c'est cette méthode qui va nous faire gagner un bon nombre de temps, il nous manque une petite fonction qui sera plus la en tant que fonction QOL (quality of life) et qui rendra juste le résultat final plus lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340940f2-3c88-4a0f-ac94-1ebb490afbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    le = LabelEncoder()\n",
    "    data_encoded = le.fit_transform(labels)\n",
    "    return data_encoded, le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c5a78-9cc9-41cb-ad0a-8afeb5e39adc",
   "metadata": {},
   "source": [
    "Bien maintenant que tout est setup nous allons pouvoir mettre en place la logique pour le modèle.\n",
    "\n",
    "<font color='#FF5733'>Les modèles ne tourneront pas sur le Jupyter Notebook pour des raisons de baisse de performance</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d224b48e-17dc-452d-85fa-220bf68f8dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score by label : ['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Positive labels precision : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\littl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb # Le modéle de multi labelling\n",
    "from sklearn.metrics import recall_score\n",
    "from statistics import mean\n",
    "\n",
    "# Pour commencer on récupére les élements\n",
    "elements = get_elements_list(\"./generation_spectre/elements\")\n",
    "\n",
    "# Génére les spectres aléatoires + les labels associés\n",
    "spectrums, labels = generate_spectrums(elements, 10)\n",
    "\n",
    "# Transforme la liste de df en 1 df\n",
    "data_shaped = shape(spectrums)\n",
    "\n",
    "# On resize la données avec pca\n",
    "data_scaled = pca(data_shaped, 5)\n",
    "\n",
    "# On split la données pour finalement essayer notre modèle\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_scaled, labels.to_numpy(), test_size=0.8, random_state = 42)\n",
    "\n",
    "model = xgb.XGBClassifier(tree_method=\"hist\")\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_val)\n",
    "\n",
    "# Calculation de précision\n",
    "overall_precision = ['%.2f' % elem for elem in recall_score(y_val, prediction, average=None)]\n",
    "positive_precision = []\n",
    "for i in range(len(y_val)):\n",
    "    for j in range(len(y_val[i])):\n",
    "        if y_val[i][j] == 1:\n",
    "            positive_precision.append(prediction[i][j])\n",
    "positive_precision = mean(positive_precision)\n",
    "\n",
    "print(f\"Score by label : {overall_precision}\")\n",
    "print(f\"Positive labels precision : {positive_precision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca988f-ef4b-4568-b96a-48a4c586afed",
   "metadata": {},
   "source": [
    "Sans surprise avec peu de données généré le score est nulle (pour le coup vraiment nulle car on est à 0) mais sur des jeux plus gros et avec plus de temps on a réussi à obtenir 11% avec 2000 spectres et 13,8% avec 7000 spectres.\n",
    "Mais on a utiliser une deuxiéme méthode qui utilise plusieurs modèles, chaque modèle va travailler sur un lot de label plutôt que tous en même temps.\n",
    "\n",
    "Mettons ça en place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fba16ea-f229-4219-9ac2-91fe402a3dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive labels precision : 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from statistics import mean\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "max_labels_per_df = 5\n",
    "\n",
    "# Les premières étapes sont similaires\n",
    "# On récupére les éléments\n",
    "elements = get_elements_list(\"./generation_spectre/elements\")\n",
    "\n",
    "# Génére les spectres aléatoires + les labels associés\n",
    "spectrums, labels = generate_spectrums(elements, 10)\n",
    "\n",
    "# Transforme la liste de df en 1 df\n",
    "data_shaped = shape(spectrums)\n",
    "\n",
    "# On resize la données avec pca\n",
    "data_scaled = pca(data_shaped, 5)\n",
    "\n",
    "# On slice ensuite les labels pour que chaque modèle s'occupe d'un morceau\n",
    "list_sliced_labels = []\n",
    "\n",
    "for i in range(len(labels.columns)//max_labels_per_df):\n",
    "    list_sliced_labels.append(labels.iloc[:,i * max_labels_per_df : (i+1) * max_labels_per_df])\n",
    "\n",
    "# Add remaining columns in a last df \n",
    "list_sliced_labels.append(labels.iloc[:, -(len(labels.columns) % max_labels_per_df) :])\n",
    "\n",
    "# Model intefration : \n",
    "prevision = pd.DataFrame()\n",
    "expected = pd.DataFrame()\n",
    "\n",
    "for df in list_sliced_labels:\n",
    "    # Préparation des données\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data_scaled, df.to_numpy(), test_size=0.8, random_state=42)\n",
    "\n",
    "    # Training\n",
    "    model = XGBClassifier(tree_method=\"hist\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting\n",
    "    result = model.predict(X_val)\n",
    "    prevision = pd.concat([prevision, pd.DataFrame(result)], axis=1)\n",
    "    expected = pd.concat([expected, pd.DataFrame(y_val)], axis=1)\n",
    "\n",
    "# Finalement on calcul la précision comme avant\n",
    "prevision = prevision.to_numpy()\n",
    "expected = expected.to_numpy()\n",
    "map(float, overall_precision)\n",
    "positive_precision = []\n",
    "for i in range(len(expected)):\n",
    "    for j in range(len(expected[i])):\n",
    "        if expected[i][j] == 1:\n",
    "            positive_precision.append(prediction[i][j])\n",
    "positive_precision = mean(positive_precision)\n",
    "\n",
    "print(f\"positive labels precision : {positive_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9a49b-21e9-4e01-ba2e-cb0a4b4caa1c",
   "metadata": {},
   "source": [
    "Encore une fois un résultat nulle mais avec un jeux de données de 2000 on obtiens un score de 12% et 14% avec 7000 spectres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35220337-93a2-4013-bd74-0792258c6045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
