{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58768f7d-1138-477e-92c1-9c692954cad0",
   "metadata": {},
   "source": [
    "# Suivis Projet Info : Stellar Analytics\n",
    "#### Axelle Refeyton . Jean Roelens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2086a81-d853-428c-b0a8-26839441c19c",
   "metadata": {},
   "source": [
    "Le but de ce projet est initialement de permettre à un utilisateur de prendre une photo d'une étoile et de retrouver des informations dessus, notamment son nom, sa position ainsi que sa composition chimique qui peux être récupérée en fonction de son spectre lumineux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af42a8-2fb6-4ef2-a41b-98cb2ecd6808",
   "metadata": {},
   "source": [
    "Ressources utilisées : \n",
    "- [NIST Spectra Database](https://physics.nist.gov/PhysRefData/ASD/lines_form.html)\n",
    "- [Documentation sur les spectres lumineux](https://atomic-spectra.net/)\n",
    "- [Simbad, DB d'étoiles communautaire open source](https://simbad.u-strasbg.fr/simbad/)\n",
    "- [MAST portal](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html)\n",
    "- [Raies de Fraunhofer](https://fr.wikipedia.org/wiki/Raies_de_Fraunhofer)\n",
    "- [Plate Solving](https://nighttime-imaging.eu/docs/master/site/advanced/platesolving)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6325db-18bc-4b28-b676-bc19ea9c5709",
   "metadata": {},
   "source": [
    "En début de projet, beaucoup de temps fut consacré à la recherche de jeux de données et de documentation. Egalement, nous avons eu l'occasion de nous entretenir lors d'une conférence Zoom avec deux membres du CNES (Orphée Faucoz et Denis Standarovski) travaillant depuis plusieurs années sur un sujet similaire : L'identification d'exoplanètes en utilisant les spectres d'absorption.\n",
    "\n",
    "Au cours de l'entretien ils nous ont clairement expliqué que ce que l'on souhaiter faire était une tâche difficile pour laquelle il n'y avait pas beaucoup de ressources, et que cela nécessiterait beaucoup de temps ne serait-ce que pour la compréhension de l'environnement dans lequel nous naviguons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fc5e2-2628-441a-87cd-2f9d22d8b6e8",
   "metadata": {},
   "source": [
    "Nous avons parcouru beaucoup de bases de données (certaines gratuites et d'autres non) afin de trouver celles qui correspondrait au mieux à nos besoins. Au final nous avons retenu Simbad pour la récupération des données générales des étoiles et NIST pour récupérer le spectre d'émissions des élèments que peuvent composer une étoile, c'est-à-dire des gaz et des métaux. Malheureusement, aucune base ne nous permet de récupérer un grand nombre de spectres d'étoiles efficacement : il va donc falloir les générer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a24c9c-0c4e-44ef-b8a0-8c14af6a3864",
   "metadata": {},
   "source": [
    "Devant la complexité de la tâche, nous avons décidé de découper le projet en différentes partie et de ce concentrer pour le moment sur la partie spectre chimique. Cette tâche a été divisée en plusieurs étapes : la récupération des spectres des éléments, et la génération de spectres d'étoiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac218d2a-038a-451d-8f66-eecd2f59a138",
   "metadata": {},
   "source": [
    "## Spectre chimique d'élément (J.Roelens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a406d77-7e3a-41ee-b726-c2fa84c8570c",
   "metadata": {},
   "source": [
    "L'objectif de cette partie est :\n",
    "1. **Pour chaque éléments récupérer les différentes ionisations possible.**\n",
    "2. **Extraire le tableau de spectres depuis NIST (en utilisant un scrapper)**\n",
    "3. **Le formatter pour ne récupérer que les lignes fortes**\n",
    "4. **Créer un dataset par élément et ionization avec les lignes fortes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966b437-d2a5-4089-a17e-b215984f6c32",
   "metadata": {},
   "source": [
    "Pour commencer, il faut que je récupére les ionisations de chaque éléments. Etant une étape pas forcément très complexe,j'ai préféré le faire à la main en m'aidant de [atomic-spectra](https://atomic-spectra.net/spectrum.php) et des [Raies de Fraunhofer](https://fr.wikipedia.org/wiki/Raies_de_Fraunhofer). On en ressort cette liste : \n",
    "- Oxygène [I, II, III, IV, V, VI, VII, VIII]\n",
    "- Hydrogènes [I]\n",
    "- Sodium [I, II, III, IV, V, VI, VII, VIII, IX, X, XI]\n",
    "- Hélium [I, II]\n",
    "- Mercure [I, II, III]\n",
    "- Fer [I, II ,III , IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI, XVII, XVIII, XIX, XX, XXI, XXII, XXIII, XXIV, XXV, XVI]\n",
    "- Magnésium [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII]\n",
    "- Calcium [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI]\n",
    "- Titane [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI, XVII, XVIII, XIX, XX, XXI, XXII]\n",
    "- Nickel [I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII, XIV, XV, XVI, XVII, XVIII, XIX, XX, XXI, XXII, XXIII, XXIV, XXV, XXVI, XXVII, XXVIII]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3d45b-1c60-44aa-8a72-110b98ef1fe0",
   "metadata": {},
   "source": [
    "Maintenant que l'étape facile est faite, je vais pouvoir m'attaquer à l'étape la plus hardue : extraire les données depuis NIST. L'idée globale est de récupérer le HTML de la page NIST en donnant dans l'URL du site l'élément ainsi que son indice de ionisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64242ce2-f0d6-48bb-a0e0-e8622f0c863b",
   "metadata": {},
   "source": [
    "En saisissant H I (pour Hydrogène Ionization 1) on obtiens cet URL :\n",
    "\n",
    "https://physics.nist.gov/cgi-bin/ASD/lines1.pl?spectra=H+I&output_type=0&low_w=&upp_w=&unit=1&submit=Retrieve+Data&de=0&plot_out=0&I_scale_type=1&format=0&line_out=0&en_unit=0&output=0&bibrefs=1&page_size=15&show_obs_wl=1&show_calc_wl=1&unc_out=1&order_out=0&max_low_enrg=&show_av=2&max_upp_enrg=&tsb_value=0&min_str=&A_out=0&intens_out=on&max_str=&allowed_out=1&forbid_out=1&min_accur=&min_intens=&conf_out=on&term_out=on&enrg_out=on&J_out=on\n",
    "\n",
    "Plutôt long, mais ce qui est assez intéressant c'est que beaucoup de paramètres pourront être passés assez facilement par celle-ci. Comparons avec l'URL de O I pour voir où saisir l'élément et l'ionization \n",
    "\n",
    "https://physics.nist.gov/cgi-bin/ASD/lines1.pl?spectra=O+I&output_type=0&low_w=&upp_w=&unit=1&submit=Retrieve+Data&de=0&plot_out=0&I_scale_type=1&format=0&line_out=0&en_unit=0&output=0&bibrefs=1&page_size=15&show_obs_wl=1&show_calc_wl=1&unc_out=1&order_out=0&max_low_enrg=&show_av=2&max_upp_enrg=&tsb_value=0&min_str=&A_out=0&intens_out=on&max_str=&allowed_out=1&forbid_out=1&min_accur=&min_intens=&conf_out=on&term_out=on&enrg_out=on&J_out=on\n",
    "\n",
    "Comme on peut le voir l'élèment se trouve en début d'URL. Essayons cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea233f17-d499-4df2-89d1-5b2b3309b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succés de la requéte\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "START_URL = r\"https://physics.nist.gov/cgi-bin/ASD/lines1.pl?spectra=\"\n",
    "END_URL = r\"&output_type=0&low_w=&upp_w=&unit=1&submit=Retrieve+Data&de=0&plot_out=0&I_scale_type=1&format=0&line_out=0&en_unit=0&output=0&bibrefs=1&page_size=15&show_obs_wl=1&show_calc_wl=1&unc_out=1&order_out=0&max_low_enrg=&show_av=2&max_upp_enrg=&tsb_value=0&min_str=&A_out=0&intens_out=on&max_str=&allowed_out=1&forbid_out=1&min_accur=&min_intens=&conf_out=on&term_out=on&enrg_out=on&J_out=on\"\n",
    "\n",
    "# Essayons de voir si on peut obtenir un code 200, mais avant cela construisons notre URL de maniére naïve pour ce test\n",
    "\n",
    "URL = START_URL + \"Ni+X\" + END_URL\n",
    "\n",
    "resp = requests.get(URL)\n",
    "if resp.status_code == 200:\n",
    "    print(f\"Succés de la requéte\")\n",
    "else :\n",
    "    print(f\"Echec de la requéte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f9639-5a69-4c45-880c-36843c45eb3a",
   "metadata": {},
   "source": [
    "Maintenant que notre approche naïve fonctionne vérifions que l'intégralité des élèments et ionization fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac395e4a-0267-466a-87a6-86253e94f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEMENT_DICT = {\n",
    "    \"O\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\"],\n",
    "    \"H\" : [\"I\"],\n",
    "    \"Na\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\"],\n",
    "    \"He\" : [\"I\",\"II\"],\n",
    "    \"Hg\" : [\"I\",\"II\",\"III\"],\n",
    "    \"Fe\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\"],\n",
    "    \"Mg\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\"],\n",
    "    \"Ca\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\"],\n",
    "    \"Ti\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\",\"XVII\",\"XVIII\",\"XIX\",\"XX\",\"XXI\",\"XXII\"],\n",
    "    \"Ni\" : [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\",\"VII\",\"VIII\",\"IX\",\"X\",\"XI\",\"XII\",\"XIII\",\"XIV\",\"XV\",\"XVI\",\"XVII\",\"XVIII\",\"XIX\",\"XX\",\"XXI\",\"XXII\",\"XXIII\",\"XXIV\",\"XXV\",\"XXVI\",\"XXVII\",\"XXVIII\"]\n",
    "}\n",
    "\n",
    "# Je vérifie la longueur de chacune des clès pour être sur de ne pas avoir fait d'erreur \n",
    "def element_dict_verification():\n",
    "    for key in ELEMENT_DICT.keys():\n",
    "        print(f\"Elément : {key} Len : {len(ELEMENT_DICT[key])}\")\n",
    "\n",
    "# Et ensuite on test les requêtes\n",
    "\n",
    "def get_200(r : requests.models.Response):\n",
    "    if r.status_code != 200:\n",
    "        return False\n",
    "    return True\n",
    "def multi_request_verification(d : dict):\n",
    "    for k in d.keys():\n",
    "        for v in range(len(d[k])):\n",
    "            url = START_URL + k + \"+\" + d[k][v] + END_URL\n",
    "            if not get_200(requests.get(url)):\n",
    "                print(f\"Error with el : {k} and ionization : {d[k][v]}\")\n",
    "            else : \n",
    "                print(f\"Success with el : {k} and ionization : {d[k][v]}\")\n",
    "# multi_request_verification(ELEMENT_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627647d4-82fe-42b8-8a00-f0e6ccbf6967",
   "metadata": {},
   "source": [
    "Ok, on dirait bien qu'on a que des codes 200 pour l'ensemble des requêtes ! Malheureusement, on se rendra compte plus tard que NIST renvoie des code 200 même en cas de mauvais URL. Le seul moyen de savoir si la requête a bien abouti est de récupérer le tableau **\"\\<table>\"** qui va contenir les données qui nous intéresse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21043a9-2811-486b-98cd-4ba61a3f02ff",
   "metadata": {},
   "source": [
    "L'idée est de voir si on peut récupérer la balise \"table\" contenant le styling que nous cherchons pour chaque lien. On s'aide de la librairie que nous allons le plus utiliser durant cette partie : \"BeautifulSoup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9296ef13-88de-40be-910d-ef301b2ee57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "def url_builder(d : dict):\n",
    "    resp = {}\n",
    "    for key in d.keys():\n",
    "        for v in d[key]:\n",
    "            resp[key+v] = START_URL + key + \"+\" + v + END_URL\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f595db-6b41-43ab-a833-49a9f70a04d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with url : NiVI\n",
      "error with url : NiVIII\n"
     ]
    }
   ],
   "source": [
    "urls = url_builder(ELEMENT_DICT)\n",
    "for url in urls:\n",
    "    req = requests.get(urls[url])\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    target_table = [table for table in tables if 'background-color:#FFFEEE;' in table.get('style', '')]\n",
    "    if len(target_table) != 1:\n",
    "        print(f'error with url : {url}')\n",
    "    else :\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71520cd3-a275-4eb9-9401-4b0308d83023",
   "metadata": {},
   "source": [
    "On voit donc désormais que deux lien sont *mort* : celui pour le Nickel 6 et le Nickel 8. Hormis ça, l'ensemble des autres liens marche. Pour la prochaine étape il nous faut récupérer toute les rows contenant une donnée dans la colonne *Observed Wavelength Vac (nm)* ainsi que *Rel. Int.* Ce sont les deux colonnes qui nous intéressent pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388a7a5-8be8-4377-b7ad-1b8203d6dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       wavelength intensities chemical\n",
      "count         174         174      174\n",
      "unique        157          48        1\n",
      "top       94.8686         120      O I\n",
      "freq            3          18      174\n"
     ]
    }
   ],
   "source": [
    "# Pour commencer passont la logique précédente en fonction \n",
    "\n",
    "def get_table(req : requests.models.Response):\n",
    "    soup = BeautifulSoup(req.text,'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    target_table = [table for table in tables if 'background-color:#FFFEEE;' in table.get('style', '')]\n",
    "    if len(target_table) > 0 :\n",
    "        return target_table[0]\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "# On va traiter seulement le premier question de rapidité depuis python 3.6 les dicts sont ordonnées du coup le premier élèment sera toujours OI\n",
    "urls = url_builder(ELEMENT_DICT)\n",
    "oxygen_one = list(urls.items())[0]\n",
    "table = get_table(requests.get(oxygen_one[1]))\n",
    "# Maintenant on va \"drill down\" la table afin d'obtenir tout les Tbody puis les Tr qui nous intéressent.\n",
    "tbodies = table.find_all('tbody')\n",
    "# Seul les tbody pair nous intéresse car ils contiennent la data en question\n",
    "evenTBodies = tbodies[1::2]\n",
    "# Ensuite pour chaque tbody on veut récupérer toute les balises tr\n",
    "# firstTBody = evenTBodies[0]\n",
    "# Holder pour les values récupéré afin d'en faire un dataframe par la suite\n",
    "All_Observed_Wavelength = []\n",
    "All_Relative_Intensities = []\n",
    "\n",
    "# for tbody in evenTBodies :\n",
    "#     for tr in tbody.find_all('tr'):\n",
    "#         all_td = tr.find_all('td')\n",
    "#         if len(all_td) >= 2:\n",
    "#             Obs_Wav = all_td[0].text.strip()\n",
    "#             Rel_Int = all_td[2].text.strip()\n",
    "#             if Obs_Wav != \"\" and Rel_Int != \"\":\n",
    "#                 All_Observed_Wavelength.append(Obs_Wav)\n",
    "#                 All_Relative_Intensities.append(Rel_Int)\n",
    "# pprint(All_Observed_Wavelength)\n",
    "\n",
    "# Tiens on remarque que la structure du HTML doit avoir un petite erreur car lors du print de All_Observed_Wavelength \n",
    "# on remarque qu'on print les bonnes colonnes avec les bonnes valeur mais, on a 1 fois le tableau 1, 2 fois le tableau 2, et finalement 3 fois le tableau\n",
    "# 3 ce qui nous indique que l'on peut parcourir une seul fois le tableau 1 et récupérer toutes les données.\n",
    "\n",
    "tbody = tbodies[1]\n",
    "for tr in tbody.find_all('tr'):\n",
    "    all_td = tr.find_all('td')\n",
    "    if len(all_td) >= 2:\n",
    "        obs_wav = all_td[0].text.strip()\n",
    "        rel_int = all_td[2].text.strip()\n",
    "        if obs_wav != \"\" and rel_int != \"\":\n",
    "            All_Observed_Wavelength.append(obs_wav)\n",
    "            All_Relative_Intensities.append(rel_int)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'wavelength' : All_Observed_Wavelength,\n",
    "    'intensities': All_Relative_Intensities,\n",
    "    'chemical' : 'O I'\n",
    "})\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f12ba1-c599-458f-9b82-a09f45378a7a",
   "metadata": {},
   "source": [
    "## Génération de spectre d'étoile (A.Refeyton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946efe2-b52b-45aa-86eb-697f04d9fb8d",
   "metadata": {},
   "source": [
    "<font color='#FF5733'>Attention : cette partie comportera des blocs de code mais ceux-ci seront plus à titre indicatif que réellement fonctionnel étant donné le besoin de jeux de données importants pour que ceux-ci fonctionne.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255f917-2853-4cb7-8b18-395d62f3be7d",
   "metadata": {},
   "source": [
    "Pour la création de spectre, l'idée va être d'exploiter les données scrappées de manière à sélectionner parmis elles un nombre X d'éléments et de créer une sorte de copycat de spectres d'étoiles à partir de ces élèments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042be55-6de8-4d7e-acd7-f29fca354726",
   "metadata": {},
   "source": [
    "Pour commencer, il faut récupérer la liste des éléments précédemment scrappés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "996ac0b2-5ff5-4c79-853c-fccc79aaafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_elements_list(path):\n",
    "    \"\"\"\n",
    "    path : chemin vers le dossier contenant tout les csv éléments\n",
    "    \"\"\"\n",
    "    filenames = os.listdir(path)\n",
    "    dataframes = []\n",
    "    for n in filenames:\n",
    "        data = pd.read_csv(path + \"\\\\\" + n)\n",
    "        # On nomme le dataframe en fonction de l'élément\n",
    "        data.Name = n[:-4]\n",
    "        dataframes.append(data)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f4dd8-5d98-4884-a301-bfc9c14fad7e",
   "metadata": {},
   "source": [
    "Une fois les dataframes nommés et stockés, on va pouvoir les exploiter et générer nos spectres. On récupère d'abord les données d'un certains nombre de dataframe élément qu'on injecte ensuite dans un dataframe vide puis qu'on remplit de bruit. En parallèle, on créée un autre dataframe qui va nous permettre de stocker les labels correspondants à chaque spectre. Chaque élément possible est représenté par une colonne. On note la présence d'un élément par un 1 et son absence par un 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52b935e7-83e4-4484-8ada-9e76e950ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "# hyperparamètres\n",
    "min_elems_used = 4\n",
    "max_elems_used = 5\n",
    "min_relint_noise = 0\n",
    "max_relint_noise = 30\n",
    "min_wl_noise = 0\n",
    "max_wl_noise = 1100\n",
    "min_n_noise = 1500\n",
    "max_n_noise = 2000\n",
    "\n",
    "def generate_spectrums(base_elements, n):\n",
    "    \"\"\"\n",
    "    base_elements : Ensemble des dataframes éléments\n",
    "    n : nombre de dataframe à générer\n",
    "    \"\"\"\n",
    "    max_elems = len(base_elements)\n",
    "    list_df = []\n",
    "    matrice_labels = pd.DataFrame(columns=[x.Name for x in base_elements]) \n",
    "\n",
    "    # On vérifie qu'il y ait assez d'éléments\n",
    "    if max_elems < min_elems_used:\n",
    "        raise ValueError(f'Not enough data, {max_elems} elements present')\n",
    "    \n",
    "    for i in range(n):\n",
    "        # création du df resultat en y ajoutant un nombre aléatoire d'éléments de base compris entre 2 et 5\n",
    "        rand_elems = rand.choices(base_elements, k=rand.randrange(min_elems_used, max_elems if max_elems < max_elems_used else max_elems_used))\n",
    "        # initialisation des colonnes\n",
    "        matrice_labels.loc[i] = [0] * max_elems\n",
    "\n",
    "        # Changement du 0 en 1 pour les éléments présents\n",
    "        for elem in rand_elems:\n",
    "            matrice_labels.at[i, elem.Name] = 1\n",
    "            \n",
    "        df_res = pd.concat(rand_elems, ignore_index=True)\n",
    "\n",
    "        # Génération du bruit\n",
    "        n_noise = rand.randint(1500, 2000)\n",
    "        # Génération des wavelengths pour chacun des bruits\n",
    "        rand_wls = [rand.uniform(0,1100) for x in range(n_noise)]\n",
    "\n",
    "        # Ajout du bruit au dataset existant\n",
    "        for n in range(0,n_noise):\n",
    "            df_res.loc[len(df_res)] = {\"wavelength\": round(rand_wls[n], 1), \"relint\": round(rand.uniform(0, 50), 1)} \n",
    "        \n",
    "        list_df.append(df_res.sort_values(\"wavelength\"))\n",
    "        \n",
    "    return list_df, matrice_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad9c21-9c29-4d7d-ba35-f756f3e29a66",
   "metadata": {},
   "source": [
    "On récupère ainsi une liste de dataframe représentant chacun un spectre ainsi qu'un dataframe contenant les labels correspondants.\n",
    "Il est maintenant temps d'exploiter ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bac455-bff9-4643-bb58-4da5843d6030",
   "metadata": {},
   "source": [
    "## Exploitation des spectres construis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48889965-5734-49f3-b1c3-5bfd932d229c",
   "metadata": {},
   "source": [
    "Attaquons-nous à la partie qui nous intéresse : exploiter les spectres afin de déterminer s'il est possible de trouver les différents éléments qui le composent\n",
    "\n",
    "Pour ce faire, nous allons devoir utiliser un type de modèle un peu particulier : la classification supervisée multi-label. Il s'agit d'un type de modèle capable de classer une observation en lui affectant un à plusieurs labels différents, à savoir nos éléments.\n",
    "\n",
    "Mais avant d'intégrer le modèle, il faut façonner nos données en un format que notre modèle peut digérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f025940-2537-4e6d-a747-2e50469c0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import warnings # Permet d'ignorer certains warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77238f9-2433-4a79-8941-68054b131daf",
   "metadata": {},
   "source": [
    "La première étape va être d'\"aplatir\" nos dataframes. On récupère chaque cellule contenant de la donnée et on l'ajoute à une liste, qu'on ajoute ensuite à un dataframe contenant tout les résultats. De cette manière, une ligne de ce dataframe contiendra toutes les données relatives à un spectre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459702ee-c7c6-48da-8deb-5679256464ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(list_df):\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    for df in list_df:\n",
    "        # Aplatir le df en liste de x valeurs\n",
    "        data_flat = df.to_numpy().flatten()\n",
    "\n",
    "        # ajouter des colonnes au dataframe s'il n'y en a pas assez\n",
    "        while len(data_flat) > len(result_df.columns):\n",
    "            result_df[len(result_df.columns)] = 0\n",
    "\n",
    "        # Si le df aplatis à moins de colonnes que le df de résultat on rajoute des colonnes au df aplatis et on y attribue la valeur nan (0)\n",
    "        if len(data_flat) < len(result_df.columns):\n",
    "            data_flat = np.concatenate([data_flat, [0] * (len(result_df.columns) - len(data_flat))])\n",
    "        \n",
    "        # ajout de la ligne\n",
    "        result_df.loc[len(result_df)] = data_flat\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916411c-5bef-48e5-b7c2-08d6b88aa47e",
   "metadata": {},
   "source": [
    "Le problème, c'est qu'on se retrouve avec un dataframe contenant parfois des dizaines de milliers de colonnes. Afin d'y remédier, on a recours a l'ACP (analyse de composants principaux). Cette technique permet de réduire drastiquement le nombre de colonnes sans perdre les relations entre les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24b1278-b5cd-4108-b4eb-938832ee330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(df, nb_components):\n",
    "\n",
    "    # Transforme le dataframe pour que les valeurs soit comprise en 0 et 1\n",
    "    data_scaled = MinMaxScaler().fit(df).transform(df)\n",
    "\n",
    "    # PCA \n",
    "    result_df = PCA(n_components=nb_components).fit_transform(data_scaled)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc78fd-b9c3-4a0c-9c3a-8e70081c3a08",
   "metadata": {},
   "source": [
    "Pas très impressionant en effet ! Et pourtant c'est cette méthode qui va nous faire gagner un temps considérable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c5a78-9cc9-41cb-ad0a-8afeb5e39adc",
   "metadata": {},
   "source": [
    "Bien ! Maintenant que tout est prêt nous allons pouvoir intégrer le modèle.\n",
    "\n",
    "<font color='#FF5733'>Les modèles ne tourneront pas sur le Jupyter Notebook pour des raisons de baisse de performance</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d224b48e-17dc-452d-85fa-220bf68f8dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score by label : ['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Positive labels precision : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\littl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb # Le modèle de multi labelling\n",
    "from sklearn.metrics import recall_score\n",
    "from statistics import mean\n",
    "\n",
    "# Pour commencer on récupére les élements\n",
    "elements = get_elements_list(\"./generation_spectre/elements\")\n",
    "\n",
    "# Génére les spectres aléatoires + les labels associés\n",
    "spectrums, labels = generate_spectrums(elements, 10)\n",
    "\n",
    "# Transforme la liste de df en 1 df\n",
    "data_shaped = shape(spectrums)\n",
    "\n",
    "# On resize la données avec pca\n",
    "data_scaled = pca(data_shaped, 5)\n",
    "\n",
    "# On split la données pour finalement essayer notre modèle\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_scaled, labels.to_numpy(), test_size=0.8, random_state = 42)\n",
    "\n",
    "model = xgb.XGBClassifier(tree_method=\"hist\")\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_val)\n",
    "\n",
    "# Calculation de précision\n",
    "overall_precision = ['%.2f' % elem for elem in recall_score(y_val, prediction, average=None)]\n",
    "positive_precision = []\n",
    "for i in range(len(y_val)):\n",
    "    for j in range(len(y_val[i])):\n",
    "        if y_val[i][j] == 1:\n",
    "            positive_precision.append(prediction[i][j])\n",
    "positive_precision = mean(positive_precision)\n",
    "\n",
    "print(f\"Score by label : {overall_precision}\")\n",
    "print(f\"Positive labels precision : {positive_precision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca988f-ef4b-4568-b96a-48a4c586afed",
   "metadata": {},
   "source": [
    "Sans surprise avec peu de données générées le score est nul (pour le coup vraiment nul car on est à 0) mais sur des jeux plus gros et avec plus de temps, on obtiens de 11% (avec 2000 spectres) à 13,8% (avec 7000 spectres).\n",
    "Le résultat n'étant pas satisfaisant, on décide d'utiliser une deuxième méthode d'implémentation utilisant plusieurs modèles.\n",
    "\n",
    "Le principe est de séparer les différents labels à trouver et de les répartir entre plusieurs modèles afin d'alléger la charge de travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fba16ea-f229-4219-9ac2-91fe402a3dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive labels precision : 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from statistics import mean\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "max_labels_per_df = 5\n",
    "\n",
    "# Les premières étapes sont similaires\n",
    "# On récupére les éléments\n",
    "elements = get_elements_list(\"./generation_spectre/elements\")\n",
    "\n",
    "# Génére les spectres aléatoires + les labels associés\n",
    "spectrums, labels = generate_spectrums(elements, 10)\n",
    "\n",
    "# Transforme la liste de df en 1 df\n",
    "data_shaped = shape(spectrums)\n",
    "\n",
    "# On resize la données avec pca\n",
    "data_scaled = pca(data_shaped, 5)\n",
    "\n",
    "# On slice ensuite les labels pour que chaque modèle s'occupe d'un morceau\n",
    "list_sliced_labels = []\n",
    "\n",
    "for i in range(len(labels.columns)//max_labels_per_df):\n",
    "    list_sliced_labels.append(labels.iloc[:,i * max_labels_per_df : (i+1) * max_labels_per_df])\n",
    "\n",
    "# Sans oublier les dernières colonnes\n",
    "list_sliced_labels.append(labels.iloc[:, -(len(labels.columns) % max_labels_per_df) :])\n",
    "\n",
    "# On instancie un modèle pour chaque groupe de label\n",
    "prevision = pd.DataFrame()\n",
    "expected = pd.DataFrame()\n",
    "\n",
    "for df in list_sliced_labels:\n",
    "    # Préparation des données\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data_scaled, df.to_numpy(), test_size=0.8, random_state=42)\n",
    "\n",
    "    # Training\n",
    "    model = XGBClassifier(tree_method=\"hist\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting\n",
    "    result = model.predict(X_val)\n",
    "    prevision = pd.concat([prevision, pd.DataFrame(result)], axis=1)\n",
    "    expected = pd.concat([expected, pd.DataFrame(y_val)], axis=1)\n",
    "\n",
    "# Finalement, on calcule la précision comme avant\n",
    "prevision = prevision.to_numpy()\n",
    "expected = expected.to_numpy()\n",
    "map(float, overall_precision)\n",
    "positive_precision = []\n",
    "for i in range(len(expected)):\n",
    "    for j in range(len(expected[i])):\n",
    "        if expected[i][j] == 1:\n",
    "            positive_precision.append(prediction[i][j])\n",
    "positive_precision = mean(positive_precision)\n",
    "\n",
    "print(f\"positive labels precision : {positive_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9a49b-21e9-4e01-ba2e-cb0a4b4caa1c",
   "metadata": {},
   "source": [
    "Encore une fois un résultat nul avec 10 spectres, mais avec un jeux de données de 2000 puis 7000, on obtient respectivement 12% et 14%.\n",
    "\n",
    "## Révision de la mise en forme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb24d2e",
   "metadata": {},
   "source": [
    "Le résultat est très décevant. Pour environ 10h de génération de donnée et un total de 7000 spectres, on n'atteint même pas les 20% de précision. En explorant la donnée on se rend compte qu le nombre de valeurs de longueur d'onde est beaucoup trop élevé (les longueurs d'ondes allant parfois jusqu'au 4ème chiffre après la virule), ce qui noie le modèle d'informations inutiles.\n",
    "\n",
    "Nous décidons donc de changer notre approche.\n",
    "\n",
    "Plutôt que de récupérer toutes les longueurs d'onde des éléments et d'en générer de nouvelles complètement aléatoirement, nous avons décidé de définir à l'avance sur quelles longueurs d'onde travailler.\n",
    "Concrètement, on \"résume\" la donnée afin de réduire les informations parasites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2da261",
   "metadata": {},
   "source": [
    "Reprenons notre fonction *generate_spectrums* et ajoutons-y un \"pas\" permettant de déterminer les longueurs d'ondes à étudier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f65b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_generate_spectrums(base_elements, n, step = 0.5):\n",
    "    # ne peut pas être construit avec moins d' 1 element\n",
    "    nb_elems = len(base_elements)\n",
    "\n",
    "    if (nb_elems < min_elems_used):\n",
    "        print(\"not enough base elements provided\")\n",
    "        pass\n",
    "\n",
    "    # création du df labels\n",
    "    labels = pd.DataFrame(data = np.zeros((n, nb_elems)), columns=[elem.Name for elem in base_elements])\n",
    "\n",
    "    # génération des spectres\n",
    "    list_df = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # création du squelette + pré-remplissage avec du bruit\n",
    "        array_wavelength = np.arange(0,max_wl_noise,step)\n",
    "        array_relint = np.random.randint(min_relint_noise,max_relint_noise, array_wavelength.shape)\n",
    "        df = pd.DataFrame(data ={\"wavelength\": array_wavelength , \"relint\": array_relint})\n",
    "\n",
    "        # selection des éléments a rajouter\n",
    "        rand_elems = rand.choices(base_elements, k=rand.randrange(min_elems_used, nb_elems if nb_elems < max_elems_used else max_elems_used))\n",
    "\n",
    "        # loren ipsum\n",
    "        for elem in rand_elems:\n",
    "            # stockage des labels\n",
    "            labels.at[i, elem.Name] = 1\n",
    "\n",
    "            # loren ipsum\n",
    "            elem[\"wavelength\"] = elem[\"wavelength\"].apply(lambda x: myround(x, step))\n",
    "\n",
    "            # ajout au df\n",
    "            df = pd.concat([df, elem], ignore_index=True)\n",
    "        \n",
    "        # traitement des doublons de wavelength\n",
    "        df = df.sort_values(\"relint\", ascending=True).drop_duplicates([\"wavelength\"], keep=\"last\").sort_values(\"wavelength\", ascending=True).reset_index(drop=True)\n",
    "        list_df.append(df)\n",
    "\n",
    "    return list_df, labels\n",
    "\n",
    "\n",
    "def myround(n, base):\n",
    "    return base * round(n/base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49010a",
   "metadata": {},
   "source": [
    "Un pas de 0,5 nous permet de générer des longueurs d'ondes allant de 0 à 1100 séparées de 0,5 unités. On intègre nos éléments de manière à ce que la valeur la plus haute soit conservée à chaque pas.\n",
    "Comme avant, on retourne notre liste de dataframe ainsi que nos labels.\n",
    "\n",
    "Maintenant que nos spectres contiennent tous le même nombre de valeurs, la suite est beaucoup plus simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(df_list):\n",
    "    '''\n",
    "    stocke la donnée en np array 3D\n",
    "    '''\n",
    "    np_array = np.array(list(map(lambda x: x.to_numpy(), df_list)))\n",
    "    print(np_array.shape)\n",
    "\n",
    "    return np_array\n",
    "\n",
    "def pca_2D(np_array, n):\n",
    "    '''\n",
    "    np_array : une array numpy de dimension 3\n",
    "    n : le nombre de colonnes finales\n",
    "    '''\n",
    "    # flatten\n",
    "    data_2d = np.array([features_2d.flatten() for features_2d in np_array])\n",
    "    # min max scaler\n",
    "    data_scaled = MinMaxScaler().fit(data_2d).transform(data_2d)\n",
    "    # PCA\n",
    "    data_pca = PCA(n_components=n).fit_transform(data_scaled)\n",
    "\n",
    "    print(\"original shape:   \", data_2d.shape)\n",
    "    print(\"transformed shape:\", data_pca.shape)\n",
    "\n",
    "    return data_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565a618",
   "metadata": {},
   "source": [
    "On passe de plusieurs heures de génération à quelques secondes, avec une donnée dont le sens est beaucoup plus clair.\n",
    "\n",
    "Plus qu'à nourrir nos modèles :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95172d69",
   "metadata": {},
   "source": [
    "Avec 2000 spectres, on obtient un résultat de 21% de précision ! La barre des 20% est passée, mais est-ce que l'ont peut aller encore plus loin ?\n",
    "\n",
    "En réduisant le nombre d'éléments à trouver à 10, on passe à 88% de précision. Avec plus de données d'entraînement, le modèle serait donc capable de déterminer avec précision quels éléments composent chimiquement une étoile à la simple vue de son spectre d'absorption."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
